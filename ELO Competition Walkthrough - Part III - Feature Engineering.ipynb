{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> ELO Merchant Category Recommendation </h1>\n",
    "<h2> Part III - Feature Engineering </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "\n",
    "# ML Packages\n",
    "import lightgbm as lgb\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "\n",
    "# plotting packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(testing=True):\n",
    "    \n",
    "    print('Loading Data...')\n",
    "    print(f'Testing set to {testing}')\n",
    "    \n",
    "    start = time.time()\n",
    "    train_test_dtypes = {'feature_1':'int16',\n",
    "                        'feature_2':'int16',\n",
    "                        'feature_3':'int16'}\n",
    "    \n",
    "    hist_dtypes = {'authorized_flag':'str',\n",
    "                   'card_id':'str',\n",
    "                   'city_id':'int16',\n",
    "                   'installments':'int16',\n",
    "                   'category_3':'str',\n",
    "                   'merchant_category_id':'int16',\n",
    "                   'merchant_id':'str',\n",
    "                   'purchase_amount':'float',\n",
    "                   'state_id':'int16',\n",
    "                   'subsector_id':'int16'}\n",
    "    if testing:\n",
    "        n = 1000000\n",
    "    else:\n",
    "        n=None\n",
    "\n",
    "    print('Loading Train Set...1/4',end='\\r')\n",
    "    train_df = pd.read_csv(\"train.csv\",dtype=train_test_dtypes,parse_dates=['first_active_month'],nrows=n)\n",
    "    print(' '*100,end='\\r',flush=True)\n",
    "    print('Loading Test Set...2/4',end='\\r')\n",
    "    test_df = pd.read_csv(\"test.csv\",dtype=train_test_dtypes,parse_dates=['first_active_month'],nrows=n)\n",
    "    print(' '*100,end='\\r',flush=True)\n",
    "    print('Loading New Merchant Transactions...3/4',end='\\r',flush=True)\n",
    "    new_trans_df = pd.read_csv('new_merchant_transactions.csv',dtype=hist_dtypes,parse_dates=True,nrows=n)\n",
    "    print(' '*100,end='\\r',flush=True)\n",
    "    print('Loading Merchant Transactions...4/4')\n",
    "    hist_df = pd.read_csv(\"historical_transactions.csv\",dtype=hist_dtypes,parse_dates=True,nrows=n)\n",
    "    print(' '*50,end='\\r')\n",
    "    print('Data Successfully Loaded')\n",
    "    print(f'Time Taken: {time.time()-start:.2f} seconds')\n",
    "    print('-'*50)\n",
    "    return train_df,test_df,new_trans_df,hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_train_test(train_df,test_df):\n",
    "    fill = test_df.loc[:,'first_active_month'].mode().values[0]\n",
    "    test_df['first_active_month'].fillna(fill,inplace=True)\n",
    "    \n",
    "    for df in [train_df,test_df]:\n",
    "        df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "        df['month'] = df['first_active_month'].dt.month\n",
    "        df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
    "        \n",
    "    return train_df,test_df\n",
    "\n",
    "def fill_all_nans(df):\n",
    "    \n",
    "    print('Filling NaNs...1/5',end='\\r')\n",
    "    # Replace Values with NaNs\n",
    "    df['installments'] = df['installments'].replace(999,np.nan)\n",
    "    df['installments'] = df['installments'].fillna(df['installments'].mean())\n",
    "    df['installments'].fillna(df['installments'].mean(),inplace=True)\n",
    "\n",
    "    # Fill with Mode for Categorical Columns\n",
    "    fill_neg1_cols = ['city_id',\n",
    "                      'merchant_category_id',\n",
    "                      'state_id',\n",
    "                      'subsector_id',\n",
    "                      'category_3',\n",
    "                      'category_2']\n",
    "    \n",
    "    for col in fill_neg1_cols:\n",
    "        df[col] = df[col].replace(-1,np.nan)\n",
    "        fill = df.loc[:,col].mode().values[0]\n",
    "        df[col].fillna(fill,inplace=True)\n",
    "    return df\n",
    "\n",
    "def encode_categorical_features(df):\n",
    "    print('Encoding Categorical Features...2/5',end='\\r')\n",
    "    # Encode Categorical Variables\n",
    "    df['purchase_amount'] = np.round(df['purchase_amount'] / 0.00150265118 + 497.06,2)\n",
    "    df['category_1'] = df['category_1'].map({'Y':1,'N':0}).astype('bool')\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y':1,'N':0}).astype('bool')\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_dt_features(df):\n",
    "    print('Creating Date Time Features...3/5',end='\\r')\n",
    "    # Create Date Time Features\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['year'] = df['purchase_date'].dt.year.astype('int16')\n",
    "    df['month'] = df['purchase_date'].dt.month.astype('int16')\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear.astype('int16')\n",
    "    df['day'] = df['purchase_date'].dt.day.astype('int16')\n",
    "    df['dayofweek'] = df['purchase_date'].dt.dayofweek.astype('int16')\n",
    "    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype('bool')\n",
    "    df['hour'] = df['purchase_date'].dt.hour.astype('int16')\n",
    "    df['month_diff'] = (((datetime.datetime.today()-df['purchase_date']).dt.days)//30).astype('int16')\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    return df\n",
    "\n",
    "def create_additional_features(df):\n",
    "    print('Creating Additional Features...4/5',end='\\r')\n",
    "    last_hist_date = datetime.datetime(2018,2,28)\n",
    "    # Other Features\n",
    "    df['time_since_purchase_date'] = (last_hist_date-df['purchase_date']).dt.days\n",
    "    df['duration'] = df['purchase_amount']*df['month_diff']\n",
    "    df['amount_month_ratio'] = df['purchase_amount']/df['month_diff']\n",
    "    df['price'] = df['purchase_amount']/df['installments']\n",
    "\n",
    "def extra_cleaning_steps(new_trans_df,hist_df):\n",
    "    print('Performing Additional Cleaning Steps...5/5',end='\\r')\n",
    "    # drop authorized_flag column in new df\n",
    "    new_trans_df.drop(columns = 'authorized_flag',inplace=True)\n",
    "\n",
    "    # frequency encoding for hist_df\n",
    "    store_size = hist_df.groupby('merchant_id').size()\n",
    "    store_size = store_size/len(hist_df)\n",
    "\n",
    "    city_size = hist_df.groupby('city_id').size()\n",
    "    city_size = city_size/len(hist_df)\n",
    "\n",
    "    subsector_size = hist_df.groupby('subsector_id').size()\n",
    "    subsector_size = subsector_size/len(hist_df)\n",
    "\n",
    "    state_size = hist_df.groupby('state_id').size()\n",
    "    state_size = state_size/len(hist_df)\n",
    "\n",
    "    category_size = hist_df.groupby('merchant_category_id').size()\n",
    "    category_size = category_size/len(hist_df)\n",
    "\n",
    "    hist_df['store_size'] = hist_df['merchant_id'].map(store_size)\n",
    "    hist_df['city_size'] = hist_df['city_id'].map(city_size)\n",
    "    hist_df['subsector_size'] = hist_df['subsector_id'].map(subsector_size)\n",
    "    hist_df['state_size'] = hist_df['state_id'].map(state_size)\n",
    "    hist_df['category_size'] = hist_df['merchant_category_id'].map(category_size)\n",
    "    \n",
    "    # One Hot Encoding for Categorical Features\n",
    "    hist_df = pd.get_dummies(hist_df,columns = ['category_2','category_3'])\n",
    "    new_trans_df = pd.get_dummies(new_trans_df,columns = ['category_2','category_3'])\n",
    "    return new_trans_df,hist_df\n",
    "\n",
    "def clean_transactions(new_trans_df,hist_df):\n",
    "    for df in [new_trans_df,hist_df]:\n",
    "        df = fill_all_nans(df)\n",
    "        print(' '*50,end='\\r')\n",
    "        df = encode_categorical_features(df)\n",
    "        print(' '*50,end='\\r')\n",
    "        df = create_dt_features(df)\n",
    "        print(' '*50,end='\\r')\n",
    "        df = create_additional_features(df)\n",
    "        print(' '*50,end='\\r')\n",
    "    new_trans_df,hist_df = extra_cleaning_steps(new_trans_df,hist_df)\n",
    "    print(' '*50,end='\\r')\n",
    "    return new_trans_df,hist_df\n",
    "\n",
    "def preprocess_data(train_df,test_df,new_trans_df,hist_df):\n",
    "    start = time.time()\n",
    "    print('Preprocessing Data...')\n",
    "    train_df,test_df = clean_train_test(train_df,test_df)\n",
    "    new_trans_df,hist_df = clean_transactions(new_trans_df,hist_df)\n",
    "    print('Data Successfully Preprocessed')\n",
    "    print(f'Time Taken: {time.time()-start:.2f} seconds')\n",
    "    print('-'*50)\n",
    "    return train_df,test_df,new_trans_df,hist_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Testing set to False\n",
      "Loading Merchant Transactions...4/4                                                                 \n",
      "Data Successfully Loaded                          \n",
      "Time Taken: 139.06 seconds\n",
      "--------------------------------------------------\n",
      "Preprocessing Data...\n",
      "Data Successfully Preprocessed                    \n",
      "Time Taken: 272.74 seconds\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_df,test_df,new_trans_df,hist_df = load_data(testing=False)\n",
    "train_df,test_df,new_trans_df,hist_df = preprocess_data(train_df,test_df,new_trans_df,hist_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook starts off where our preprocessing notebook left off. We now have a \"Clean\" data set and can now start thinking about how we can use the merchant transactions dataframes with our train and test sets. Here we will get aggregates of each card_id then pass those values to our train and test with a join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating Historical Transactions...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hist_authorized_flag_mean</th>\n",
       "      <th>hist_card_id_count</th>\n",
       "      <th>hist_city_id_nunique</th>\n",
       "      <th>hist_category_1_sum</th>\n",
       "      <th>hist_category_1_mean</th>\n",
       "      <th>hist_category_1_std</th>\n",
       "      <th>hist_category_2_1.0_sum</th>\n",
       "      <th>hist_category_2_1.0_mean</th>\n",
       "      <th>hist_category_2_2.0_sum</th>\n",
       "      <th>hist_category_2_2.0_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>hist_city_size_mean</th>\n",
       "      <th>hist_state_size_min</th>\n",
       "      <th>hist_state_size_max</th>\n",
       "      <th>hist_state_size_mean</th>\n",
       "      <th>hist_subsector_size_min</th>\n",
       "      <th>hist_subsector_size_max</th>\n",
       "      <th>hist_subsector_size_mean</th>\n",
       "      <th>hist_category_size_min</th>\n",
       "      <th>hist_category_size_max</th>\n",
       "      <th>hist_category_size_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_ID_00007093c1</th>\n",
       "      <td>0.765101</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.187919</td>\n",
       "      <td>0.391965</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.187919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051407</td>\n",
       "      <td>0.012769</td>\n",
       "      <td>0.454163</td>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>0.192234</td>\n",
       "      <td>0.089125</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.190856</td>\n",
       "      <td>0.078497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_0001238066</th>\n",
       "      <td>0.975610</td>\n",
       "      <td>123</td>\n",
       "      <td>17</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.126992</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.837398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050689</td>\n",
       "      <td>0.031931</td>\n",
       "      <td>0.454163</td>\n",
       "      <td>0.380550</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>0.192234</td>\n",
       "      <td>0.114675</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.190856</td>\n",
       "      <td>0.087234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_0001506ef0</th>\n",
       "      <td>0.939394</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027416</td>\n",
       "      <td>0.041367</td>\n",
       "      <td>0.454163</td>\n",
       "      <td>0.053876</td>\n",
       "      <td>0.008898</td>\n",
       "      <td>0.192234</td>\n",
       "      <td>0.106659</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.190856</td>\n",
       "      <td>0.084181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_0001793786</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>216</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.096001</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.578704</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018768</td>\n",
       "      <td>0.007074</td>\n",
       "      <td>0.454163</td>\n",
       "      <td>0.266820</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.192234</td>\n",
       "      <td>0.087159</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.190856</td>\n",
       "      <td>0.054679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_000183fdda</th>\n",
       "      <td>0.951389</td>\n",
       "      <td>144</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.164909</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.076389</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024582</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>0.454163</td>\n",
       "      <td>0.052191</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.192234</td>\n",
       "      <td>0.068389</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.190856</td>\n",
       "      <td>0.040708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 hist_authorized_flag_mean  hist_card_id_count  \\\n",
       "card_id                                                          \n",
       "C_ID_00007093c1                   0.765101                 149   \n",
       "C_ID_0001238066                   0.975610                 123   \n",
       "C_ID_0001506ef0                   0.939394                  66   \n",
       "C_ID_0001793786                   0.875000                 216   \n",
       "C_ID_000183fdda                   0.951389                 144   \n",
       "\n",
       "                 hist_city_id_nunique  hist_category_1_sum  \\\n",
       "card_id                                                      \n",
       "C_ID_00007093c1                     4                 28.0   \n",
       "C_ID_0001238066                    17                  2.0   \n",
       "C_ID_0001506ef0                     3                  0.0   \n",
       "C_ID_0001793786                     9                  2.0   \n",
       "C_ID_000183fdda                     8                  4.0   \n",
       "\n",
       "                 hist_category_1_mean  hist_category_1_std  \\\n",
       "card_id                                                      \n",
       "C_ID_00007093c1              0.187919             0.391965   \n",
       "C_ID_0001238066              0.016260             0.126992   \n",
       "C_ID_0001506ef0              0.000000             0.000000   \n",
       "C_ID_0001793786              0.009259             0.096001   \n",
       "C_ID_000183fdda              0.027778             0.164909   \n",
       "\n",
       "                 hist_category_2_1.0_sum  hist_category_2_1.0_mean  \\\n",
       "card_id                                                              \n",
       "C_ID_00007093c1                     28.0                  0.187919   \n",
       "C_ID_0001238066                    103.0                  0.837398   \n",
       "C_ID_0001506ef0                      2.0                  0.030303   \n",
       "C_ID_0001793786                    125.0                  0.578704   \n",
       "C_ID_000183fdda                     11.0                  0.076389   \n",
       "\n",
       "                 hist_category_2_2.0_sum  hist_category_2_2.0_mean  \\\n",
       "card_id                                                              \n",
       "C_ID_00007093c1                      0.0                  0.000000   \n",
       "C_ID_0001238066                      0.0                  0.000000   \n",
       "C_ID_0001506ef0                      0.0                  0.000000   \n",
       "C_ID_0001793786                     76.0                  0.351852   \n",
       "C_ID_000183fdda                      1.0                  0.006944   \n",
       "\n",
       "                          ...             hist_city_size_mean  \\\n",
       "card_id                   ...                                   \n",
       "C_ID_00007093c1           ...                        0.051407   \n",
       "C_ID_0001238066           ...                        0.050689   \n",
       "C_ID_0001506ef0           ...                        0.027416   \n",
       "C_ID_0001793786           ...                        0.018768   \n",
       "C_ID_000183fdda           ...                        0.024582   \n",
       "\n",
       "                 hist_state_size_min  hist_state_size_max  \\\n",
       "card_id                                                     \n",
       "C_ID_00007093c1             0.012769             0.454163   \n",
       "C_ID_0001238066             0.031931             0.454163   \n",
       "C_ID_0001506ef0             0.041367             0.454163   \n",
       "C_ID_0001793786             0.007074             0.454163   \n",
       "C_ID_000183fdda             0.013220             0.454163   \n",
       "\n",
       "                 hist_state_size_mean  hist_subsector_size_min  \\\n",
       "card_id                                                          \n",
       "C_ID_00007093c1              0.095844                 0.002815   \n",
       "C_ID_0001238066              0.380550                 0.002815   \n",
       "C_ID_0001506ef0              0.053876                 0.008898   \n",
       "C_ID_0001793786              0.266820                 0.000803   \n",
       "C_ID_000183fdda              0.052191                 0.001349   \n",
       "\n",
       "                 hist_subsector_size_max  hist_subsector_size_mean  \\\n",
       "card_id                                                              \n",
       "C_ID_00007093c1                 0.192234                  0.089125   \n",
       "C_ID_0001238066                 0.192234                  0.114675   \n",
       "C_ID_0001506ef0                 0.192234                  0.106659   \n",
       "C_ID_0001793786                 0.192234                  0.087159   \n",
       "C_ID_000183fdda                 0.192234                  0.068389   \n",
       "\n",
       "                 hist_category_size_min  hist_category_size_max  \\\n",
       "card_id                                                           \n",
       "C_ID_00007093c1                0.000148                0.190856   \n",
       "C_ID_0001238066                0.000171                0.190856   \n",
       "C_ID_0001506ef0                0.000889                0.190856   \n",
       "C_ID_0001793786                0.000009                0.190856   \n",
       "C_ID_000183fdda                0.000153                0.190856   \n",
       "\n",
       "                 hist_category_size_mean  \n",
       "card_id                                   \n",
       "C_ID_00007093c1                 0.078497  \n",
       "C_ID_0001238066                 0.087234  \n",
       "C_ID_0001506ef0                 0.084181  \n",
       "C_ID_0001793786                 0.054679  \n",
       "C_ID_000183fdda                 0.040708  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregating Historical Transactions DataFrame by card_id\n",
    "\n",
    "print('Aggregating Historical Transactions...')\n",
    "\n",
    "# Create dictionary of column names and aggregation functions to use\n",
    "\n",
    "agg_func = {'authorized_flag' : ['mean'],\n",
    "            'card_id' : ['count'],\n",
    "            'city_id' : ['nunique'],\n",
    "            'category_1' : ['sum','mean','std'],\n",
    "            'category_2_1.0':['sum','mean'],\n",
    "            'category_2_2.0': ['sum', 'mean'],\n",
    "            'category_2_3.0': ['sum', 'mean'],\n",
    "            'category_2_4.0': ['sum', 'mean'],\n",
    "            'category_2_5.0': ['sum', 'mean'],\n",
    "            'category_3_A': ['sum', 'mean'],\n",
    "            'category_3_B': ['sum', 'mean'],\n",
    "            'category_3_C': ['sum', 'mean'],\n",
    "            'month':['nunique'],\n",
    "            'hour':['mean'],\n",
    "            'weekofyear':['mean','nunique'],\n",
    "            'day':['nunique',np.ptp,'mean'],\n",
    "            'dayofweek':['mean'],\n",
    "            'weekend':['sum','mean'],\n",
    "            'duration':['min','mean','max'],\n",
    "            'price':['sum','mean','max','min','var'],\n",
    "            'amount_month_ratio':['max','min',np.ptp],\n",
    "            'installments': ['sum','min','max','var','mean'],\n",
    "            'merchant_category_id':['nunique'],\n",
    "            'merchant_id':['nunique'],\n",
    "            'purchase_amount':['sum','mean','max','min','var','median'],\n",
    "            'purchase_date':['max','min',np.ptp],\n",
    "            'time_since_purchase_date':['min','max','mean'],\n",
    "            'month_lag':['min','max','mean','var',np.ptp],\n",
    "            'month_diff':['mean','min','max',np.ptp,'var'],\n",
    "            'store_size':['min','max','mean'],\n",
    "            'city_size':['min','max','mean'],\n",
    "            'state_size':['min','max','mean'],\n",
    "            'subsector_size':['min','max','mean'],\n",
    "            'category_size':['min','max','mean'],\n",
    "           }\n",
    "\n",
    "# Aggregate columns based on dictionary passed to agg function\n",
    "ghist_df = hist_df.groupby(['card_id']).agg(agg_func)\n",
    "\n",
    "# Rename columns before joining train/test set\n",
    "ghist_df.columns = ['hist_'+'_'.join(col).strip() for col in ghist_df.columns.values]\n",
    "ghist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating New Transactions DataFrame...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_card_id_count</th>\n",
       "      <th>new_city_id_nunique</th>\n",
       "      <th>new_category_1_sum</th>\n",
       "      <th>new_category_1_mean</th>\n",
       "      <th>new_category_2_1.0_sum</th>\n",
       "      <th>new_category_2_1.0_mean</th>\n",
       "      <th>new_category_2_2.0_sum</th>\n",
       "      <th>new_category_2_2.0_mean</th>\n",
       "      <th>new_category_2_3.0_sum</th>\n",
       "      <th>new_category_2_3.0_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>new_weekend_mean</th>\n",
       "      <th>new_month_lag_min</th>\n",
       "      <th>new_month_lag_max</th>\n",
       "      <th>new_month_lag_mean</th>\n",
       "      <th>new_month_lag_var</th>\n",
       "      <th>new_month_lag_ptp</th>\n",
       "      <th>new_month_diff_mean</th>\n",
       "      <th>new_month_diff_min</th>\n",
       "      <th>new_month_diff_max</th>\n",
       "      <th>new_month_diff_ptp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_ID_00007093c1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_0001238066</th>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>23</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.346154</td>\n",
       "      <td>0.235385</td>\n",
       "      <td>1</td>\n",
       "      <td>12.307692</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_0001506ef0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_0001793786</th>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>8</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>5</td>\n",
       "      <td>0.16129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.322581</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>1</td>\n",
       "      <td>16.193548</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_000183fdda</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.272727</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>1</td>\n",
       "      <td>12.727273</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 new_card_id_count  new_city_id_nunique  new_category_1_sum  \\\n",
       "card_id                                                                       \n",
       "C_ID_00007093c1                  2                    2                 0.0   \n",
       "C_ID_0001238066                 26                    7                 2.0   \n",
       "C_ID_0001506ef0                  2                    1                 0.0   \n",
       "C_ID_0001793786                 31                    7                 0.0   \n",
       "C_ID_000183fdda                 11                    2                 0.0   \n",
       "\n",
       "                 new_category_1_mean  new_category_2_1.0_sum  \\\n",
       "card_id                                                        \n",
       "C_ID_00007093c1             0.000000                       1   \n",
       "C_ID_0001238066             0.076923                      23   \n",
       "C_ID_0001506ef0             0.000000                       0   \n",
       "C_ID_0001793786             0.000000                      17   \n",
       "C_ID_000183fdda             0.000000                       0   \n",
       "\n",
       "                 new_category_2_1.0_mean  new_category_2_2.0_sum  \\\n",
       "card_id                                                            \n",
       "C_ID_00007093c1                 0.500000                       0   \n",
       "C_ID_0001238066                 0.884615                       0   \n",
       "C_ID_0001506ef0                 0.000000                       0   \n",
       "C_ID_0001793786                 0.548387                       8   \n",
       "C_ID_000183fdda                 0.000000                       0   \n",
       "\n",
       "                 new_category_2_2.0_mean  new_category_2_3.0_sum  \\\n",
       "card_id                                                            \n",
       "C_ID_00007093c1                 0.000000                       1   \n",
       "C_ID_0001238066                 0.000000                       0   \n",
       "C_ID_0001506ef0                 0.000000                       2   \n",
       "C_ID_0001793786                 0.258065                       5   \n",
       "C_ID_000183fdda                 0.000000                      11   \n",
       "\n",
       "                 new_category_2_3.0_mean         ...          \\\n",
       "card_id                                          ...           \n",
       "C_ID_00007093c1                  0.50000         ...           \n",
       "C_ID_0001238066                  0.00000         ...           \n",
       "C_ID_0001506ef0                  1.00000         ...           \n",
       "C_ID_0001793786                  0.16129         ...           \n",
       "C_ID_000183fdda                  1.00000         ...           \n",
       "\n",
       "                 new_weekend_mean  new_month_lag_min  new_month_lag_max  \\\n",
       "card_id                                                                   \n",
       "C_ID_00007093c1          0.000000                  2                  2   \n",
       "C_ID_0001238066          0.461538                  1                  2   \n",
       "C_ID_0001506ef0          0.000000                  1                  1   \n",
       "C_ID_0001793786          0.451613                  1                  2   \n",
       "C_ID_000183fdda          0.181818                  1                  2   \n",
       "\n",
       "                 new_month_lag_mean  new_month_lag_var  new_month_lag_ptp  \\\n",
       "card_id                                                                     \n",
       "C_ID_00007093c1            2.000000           0.000000                  0   \n",
       "C_ID_0001238066            1.346154           0.235385                  1   \n",
       "C_ID_0001506ef0            1.000000           0.000000                  0   \n",
       "C_ID_0001793786            1.322581           0.225806                  1   \n",
       "C_ID_000183fdda            1.272727           0.218182                  1   \n",
       "\n",
       "                 new_month_diff_mean  new_month_diff_min  new_month_diff_max  \\\n",
       "card_id                                                                        \n",
       "C_ID_00007093c1            13.000000                  13                  13   \n",
       "C_ID_0001238066            12.307692                  12                  13   \n",
       "C_ID_0001506ef0            12.000000                  12                  12   \n",
       "C_ID_0001793786            16.193548                  16                  17   \n",
       "C_ID_000183fdda            12.727273                  12                  13   \n",
       "\n",
       "                 new_month_diff_ptp  \n",
       "card_id                              \n",
       "C_ID_00007093c1                   0  \n",
       "C_ID_0001238066                   1  \n",
       "C_ID_0001506ef0                   0  \n",
       "C_ID_0001793786                   1  \n",
       "C_ID_000183fdda                   1  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate Columns based on Dictionary for new_trans_df\n",
    "print('Aggregating New Transactions DataFrame...')\n",
    "\n",
    "agg_func = {\n",
    "            'card_id' : ['count'],\n",
    "            'city_id' : ['nunique'],\n",
    "            'category_1' : ['sum','mean'],\n",
    "            'category_2_1.0':['sum','mean'],\n",
    "            'category_2_2.0': ['sum', 'mean'],\n",
    "            'category_2_3.0': ['sum', 'mean'],\n",
    "            'category_2_4.0': ['sum', 'mean'],\n",
    "            'category_2_5.0': ['sum', 'mean'],\n",
    "            'category_3_A': ['sum', 'mean'],\n",
    "            'category_3_B': ['sum', 'mean'],\n",
    "            'category_3_C': ['sum', 'mean'],\n",
    "            'month':['nunique'],\n",
    "            'weekofyear':['nunique'],\n",
    "            'day':['nunique',np.ptp,'mean'],\n",
    "            'dayofweek':['mean'],\n",
    "            'duration':['min','max'],\n",
    "            'price':['sum','mean','max','min','var'],\n",
    "            'amount_month_ratio':['max','min',np.ptp],\n",
    "            'installments': ['sum','min','max','var','mean'],\n",
    "            'merchant_category_id':['nunique'],\n",
    "            'merchant_id':['nunique'],\n",
    "            'purchase_amount':['sum','mean','max','min','var'],\n",
    "            'purchase_date':['max','min',np.ptp],\n",
    "            'time_since_purchase_date':['min','max','mean'],\n",
    "            'weekend':['sum','mean'],\n",
    "            'month_lag':['min','max','mean','var',np.ptp],\n",
    "            'month_diff':['mean','min','max',np.ptp]\n",
    "           }\n",
    "\n",
    "gnew_trans_df = new_trans_df.groupby(['card_id']).agg(agg_func)\n",
    "\n",
    "# Rename columns before joining train / test set\n",
    "gnew_trans_df.columns = ['new_'+'_'.join(col).strip() for col in gnew_trans_df.columns.values]\n",
    "gnew_trans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next features we add are based on how often each card_id makes the same purchase or purchases from the same merchant. This will likely give us some info on customer behavior that will help our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_repeat_purchases</th>\n",
       "      <th>mean_repeat_purchases</th>\n",
       "      <th>mean_merchant_purchases</th>\n",
       "      <th>mean_category_purchases</th>\n",
       "      <th>mean_monthly_purchases</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_ID_00007093c1</th>\n",
       "      <td>13</td>\n",
       "      <td>1.674157</td>\n",
       "      <td>5.137931</td>\n",
       "      <td>8.277778</td>\n",
       "      <td>74.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_0001238066</th>\n",
       "      <td>9</td>\n",
       "      <td>1.182692</td>\n",
       "      <td>1.892308</td>\n",
       "      <td>4.241379</td>\n",
       "      <td>61.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_0001506ef0</th>\n",
       "      <td>4</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>2.357143</td>\n",
       "      <td>3.473684</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 max_repeat_purchases  mean_repeat_purchases  \\\n",
       "card_id                                                        \n",
       "C_ID_00007093c1                    13               1.674157   \n",
       "C_ID_0001238066                     9               1.182692   \n",
       "C_ID_0001506ef0                     4               1.200000   \n",
       "\n",
       "                 mean_merchant_purchases  mean_category_purchases  \\\n",
       "card_id                                                             \n",
       "C_ID_00007093c1                 5.137931                 8.277778   \n",
       "C_ID_0001238066                 1.892308                 4.241379   \n",
       "C_ID_0001506ef0                 2.357143                 3.473684   \n",
       "\n",
       "                 mean_monthly_purchases  \n",
       "card_id                                  \n",
       "C_ID_00007093c1                    74.5  \n",
       "C_ID_0001238066                    61.5  \n",
       "C_ID_0001506ef0                    33.0  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf = pd.DataFrame(hist_df.groupby(['card_id','purchase_amount']).size().max(level=0))\n",
    "gdf.columns = ['max_repeat_purchases']\n",
    "gdf['mean_repeat_purchases'] = hist_df.groupby(['card_id','purchase_amount']).size().mean(level=0)\n",
    "gdf['mean_merchant_purchases'] = hist_df.groupby(['card_id','merchant_id']).size().mean(level=0)\n",
    "gdf['mean_category_purchases'] = hist_df.groupby(['card_id','merchant_category_id']).size().mean(level=0)\n",
    "gdf['mean_monthly_purchases'] = hist_df.groupby(['card_id','month_diff']).size().mean(level=0)\n",
    "gdf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have aggregated features from our historical transactions data frame we can join them with our train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging with training set...\n",
      "Merging with testing set...\n"
     ]
    }
   ],
   "source": [
    "# Merge with train and test set\n",
    "print('Merging with training set...')\n",
    "train = pd.merge(train_df,ghist_df,on='card_id',how='left')\n",
    "train = pd.merge(train,gnew_trans_df,on='card_id',how='left')\n",
    "train = pd.merge(train,gdf,on='card_id',how='left')\n",
    "\n",
    "print('Merging with testing set...')\n",
    "test = pd.merge(test_df,ghist_df,on='card_id',how='left')\n",
    "test = pd.merge(test,gnew_trans_df,on='card_id',how='left')\n",
    "test = pd.merge(test,gdf,on='card_id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular case, card_ids not present in the new_merchant dataframe will be interprited as zeros. Therefor values indicating purchase amounts/transactions will be set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Card_ids not present in new trans df were filled with zeros to represent zero purchases\n",
    "zero_sum_columns = ['new_purchase_amount_mean',\n",
    "                    'new_purchase_amount_sum',\n",
    "                    'new_purchase_amount_max',\n",
    "                    'new_duration_max',\n",
    "                    'new_duration_min',\n",
    "                    'new_amount_month_ratio_min',\n",
    "                    'new_amount_month_ratio_max',\n",
    "                    'new_card_id_count',\n",
    "                   ]\n",
    "\n",
    "for col in zero_sum_columns:\n",
    "    train[col] = train[col].fillna(0)\n",
    "    test[col] = test[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create some time based features from features in the original training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train,test]:\n",
    "    df['hist_purchase_date_uptonow'] = (datetime.datetime.today() - \n",
    "                                      df['hist_purchase_date_max']).dt.days\n",
    "    df['new_purchase_date_uptonow'] = (datetime.datetime.today() - \n",
    "                                      df['new_purchase_date_max']).dt.days\n",
    "    \n",
    "    df['quarter'] = df['first_active_month'].dt.quarter\n",
    "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
    "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
    "    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
    "    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
    "    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
    "    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
    "    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']\n",
    "    \n",
    "    dt_features = ['hist_purchase_date_max','hist_purchase_date_min',\n",
    "               'new_purchase_date_max','new_purchase_date_min','hist_purchase_date_ptp','new_purchase_date_ptp']\n",
    "    \n",
    "    # Models cannot use datetime features so they are encoded here as int64s\n",
    "    for feature in dt_features:\n",
    "        df[feature] = df[feature].astype(np.int64)*1e-9\n",
    "    \n",
    "    df['first_month'] = df['first_active_month'].dt.month\n",
    "    df['first_year'] = df['first_active_month'].dt.year\n",
    "    df.drop(columns = ['first_active_month'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create feature interactions between the new and historical merchant aggregate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train,test]:\n",
    "    # https://www.kaggle.com/mfjwr1/simple-lightgbm-without-blending\n",
    "    df['category_1_mean'] = df['new_category_1_mean']+df['hist_category_1_mean']\n",
    "    df['new_CLV'] = df['new_card_id_count'] * df['new_purchase_amount_sum'] / df['new_month_diff_mean']\n",
    "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_purchase_amount_sum'] / df['hist_month_diff_mean']\n",
    "        \n",
    "    # Ratios\n",
    "    df['transactions_ratio'] = df['new_card_id_count']/df['hist_card_id_count']\n",
    "    df['mean_purchase_ratio']  = df['new_purchase_amount_mean']/df['hist_purchase_amount_mean']\n",
    "    df['max_purchase_ratio'] = df['new_purchase_amount_max']/df['hist_purchase_amount_max']\n",
    "    df['sum_purchase_amount_ratio'] = df['new_purchase_amount_sum'] / df['hist_purchase_amount_sum']\n",
    "    df['mean_month_lag_ratio'] = df['new_month_lag_mean']/df['hist_month_lag_mean']\n",
    "    df['mean_month_diff_ratio'] = df['new_month_diff_mean'] / df['hist_month_diff_mean']\n",
    "    df['sum_installments_ratio'] = df['new_installments_sum'] / df['hist_installments_sum']\n",
    "    df['mean_installments_ratio'] = df['new_installments_mean'] / df['hist_installments_mean']\n",
    "    df['min_duration_ratio'] = df['new_duration_min']/df['hist_duration_min']\n",
    "\n",
    "    # Products\n",
    "    df['transactions_product'] = df['new_card_id_count']*df['hist_card_id_count']\n",
    "    df['mean_purchase_product']  = df['new_purchase_amount_mean']*df['hist_purchase_amount_mean']\n",
    "    df['max_purchase_product'] = df['new_purchase_amount_max']*df['hist_purchase_amount_max']\n",
    "    df['sum_purchase_amount_product'] = df['new_purchase_amount_sum']*df['hist_purchase_amount_sum']\n",
    "    df['mean_month_lag_product'] = df['new_month_lag_mean']*df['hist_month_lag_mean']\n",
    "    df['mean_month_diff_product'] = df['new_month_diff_mean']*df['hist_month_diff_mean']\n",
    "    df['sum_installments_product'] = df['new_installments_sum']*df['hist_installments_sum']\n",
    "    df['mean_installments_product'] = df['new_installments_mean']*df['hist_installments_mean']\n",
    "    df['min_duration_product'] = df['new_duration_min']*df['hist_duration_min']\n",
    "    \n",
    "    # Weighted Time Features\n",
    "    df['hist_min_duration_weighted'] = df['hist_duration_min']*df['hist_card_id_count']\n",
    "    df['hist_max_duration_weighted'] = df['hist_duration_max']*df['hist_card_id_count']\n",
    "    df['hist_time_since_purchase_date_min_weighted'] = df['hist_time_since_purchase_date_min']*df['hist_card_id_count']\n",
    "    \n",
    "    df['new_min_duration_weighted'] = df['new_duration_min']*df['new_card_id_count']\n",
    "    df['new_max_duration_weighted'] = df['new_duration_max']*df['new_card_id_count']\n",
    "    df['new_time_since_purchase_date_min_weighted'] = df['new_time_since_purchase_date_min']*df['new_card_id_count']\n",
    "    \n",
    "    # Additional Features\n",
    "    df['sum_price_ratio'] = df['new_price_sum'] / df['hist_price_sum']\n",
    "    df['mean_price_ratio'] = df['new_price_mean'] / df['hist_price_mean']\n",
    "    df['CLV_Ratio'] = df['new_CLV']/df['hist_CLV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes all the feature engineering done for this project. We will now start training models to evaluate performance and eliminate useless features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('final_train.csv',index=False)\n",
    "test.to_csv('final_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lgbmenv]",
   "language": "python",
   "name": "conda-env-lgbmenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
